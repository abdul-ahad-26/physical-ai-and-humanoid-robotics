from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict, Any
from ...agent.rag_agent import RAGAgent
from ...db.postgres_client import PostgresClient
from ...agent.orchestrator import MainOrchestratorAgent
from ...api.logging import get_logger
import uuid
import time
import re


router = APIRouter()

# Initialize logger
logger = get_logger()

# Request models
class AnswerRequest(BaseModel):
    question: str = Field(..., min_length=1, max_length=10000, description="The user's question about textbook content")
    k: Optional[int] = Field(3, ge=1, le=10, description="Number of content chunks to retrieve for context")
    highlight_override: Optional[str] = Field(None, min_length=1, max_length=5000, description="Optional highlighted text that replaces search context")

    @validator('question')
    def validate_question(cls, v):
        if not v or not v.strip():
            raise ValueError('Question cannot be empty')

        # Check for potentially malicious patterns
        if re.search(r'<script|javascript:|on\w+\s*=', v, re.IGNORECASE):
            raise ValueError('Question contains invalid characters or patterns')

        # Remove any HTML tags that might be malicious
        sanitized = re.sub(r'<[^>]*>', '', v)
        return sanitized.strip()

    @validator('highlight_override')
    def validate_highlight_override(cls, v):
        if v is not None:
            # Check for potentially malicious patterns
            if re.search(r'<script|javascript:|on\w+\s*=', v, re.IGNORECASE):
                raise ValueError('Highlight override contains invalid characters or patterns')

            # Remove any HTML tags that might be malicious
            sanitized = re.sub(r'<[^>]*>', '', v)
            return sanitized.strip()

        return v


# Response models
class RetrievedContext(BaseModel):
    content: str
    metadata: Dict[str, Any]
    score: float


class AnswerResponse(BaseModel):
    answer: str
    retrieved_contexts: List[RetrievedContext]
    confidence_score: float
    answer_id: str


@router.post("/answer",
             response_model=AnswerResponse,
             summary="Generate answer to textbook question",
             description="Accepts a user question and returns a final natural-language answer generated by the AI agent. Performance target: Complete within 2 seconds.")
async def answer_endpoint(request: AnswerRequest, background_tasks: BackgroundTasks):
    """
    Answer endpoint that accepts a user question and returns a final natural-language answer
    generated by the AI agent.
    Performance target: Complete within 2 seconds

    Args:
        request (AnswerRequest): The answer request containing the question and optional parameters
        background_tasks (BackgroundTasks): Background tasks for logging

    Returns:
        AnswerResponse: Contains the generated answer, retrieved contexts, confidence score, and answer ID

    Raises:
        HTTPException: If there are validation errors or service unavailability
    """
    start_time = time.time()
    request_id = str(uuid.uuid4())

    try:
        # Log incoming request
        logger.info(f"Answer request received", {
            "request_id": request_id,
            "question_length": len(request.question),
            "k_value": request.k,
            "has_highlight_override": request.highlight_override is not None
        })

        # Initialize orchestrator agent
        orchestrator_agent = MainOrchestratorAgent()

        # Set a timeout to ensure we stay under 2 seconds
        # Use 1.8 seconds to leave some buffer for processing overhead
        import asyncio

        # Run the query processing with a timeout
        start_time = time.time()
        try:
            result = await asyncio.wait_for(
                asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: orchestrator_agent.process_query(
                        query=request.question,
                        k=request.k,
                        highlight_override=request.highlight_override
                    )
                ),
                timeout=1.8
            )
            processing_time = time.time() - start_time

            # Log agent execution
            logger.log_agent_execution(
                agent_name="MainOrchestratorAgent",
                operation="process_query",
                execution_time=processing_time,
                success=True
            )

            # Ensure the result has the expected format
            if not isinstance(result, dict):
                logger.error(f"Unexpected result format from orchestrator_agent.process_query", {
                    "request_id": request_id,
                    "result_type": type(result).__name__
                })
                raise HTTPException(status_code=500, detail={
                    "error": "Invalid response format",
                    "details": "Orchestrator returned unexpected result format",
                    "request_id": request_id
                })

        except asyncio.TimeoutError:
            processing_time = time.time() - start_time
            # If we timeout, try a simpler/faster approach
            logger.warning(f"Query processing timed out", {
                "request_id": request_id,
                "processing_time": round(processing_time, 3),
                "timeout_limit": 1.8
            })

            # Use a faster fallback - direct retrieval without full orchestration
            rag_agent = RAGAgent()
            retrieved_contexts = rag_agent.retrieve_content(request.question, k=min(request.k, 2))  # Use fewer results for speed

            # Generate a simple answer based on retrieved contexts
            if retrieved_contexts:
                answer = f"Based on the textbook: {retrieved_contexts[0]['content'][:300]}... [Answer generated quickly due to high load]"
                confidence_score = min(0.7, retrieved_contexts[0]['score'])  # Lower confidence for fallback
            else:
                answer = "I couldn't find specific information in the textbook to answer your question quickly."
                confidence_score = 0.2

            result = {
                "answer": answer,
                "retrieved_contexts": retrieved_contexts,
                "confidence_score": confidence_score,
                "assembled_context": f"Quick context for: {request.question}",
                "query_id": f"quick_{uuid.uuid4().hex[:8]}",
                "answer_id": f"quick_ans_{uuid.uuid4().hex[:8]}"
            }

        # Extract the answer and other data from the result
        answer = result["answer"]
        retrieved_contexts = result["retrieved_contexts"]
        confidence_score = result["confidence_score"]

        # Validate the response before creating the response object
        if not answer or not isinstance(answer, str):
            print(f"ERROR: Invalid answer format for request {request_id}")
            raise HTTPException(status_code=500, detail={
                "error": "Invalid response format",
                "details": "Generated answer is invalid"
            })

        if not isinstance(retrieved_contexts, list):
            print(f"ERROR: Invalid context format for request {request_id}")
            raise HTTPException(status_code=500, detail={
                "error": "Invalid response format",
                "details": "Retrieved contexts format is invalid"
            })

        # Create response
        response = AnswerResponse(
            answer=answer,
            retrieved_contexts=[
                RetrievedContext(
                    content=ctx["content"],
                    metadata=ctx["metadata"],
                    score=ctx["score"]
                ) for ctx in retrieved_contexts
            ],
            confidence_score=confidence_score,
            answer_id=result["answer_id"]
        )

        # Log the user session in the background (non-blocking)
        def log_session():
            log_start = time.time()
            try:
                postgres_client = PostgresClient()
                session_id = postgres_client.log_user_session(
                    query=request.question,
                    response=answer,
                    retrieved_context=retrieved_contexts,
                    session_metadata={
                        "highlight_override": request.highlight_override,
                        "k_value": request.k,
                        "processing_time": time.time() - start_time,
                        "endpoint": "answer",
                        "was_timeout": time.time() - start_time > 1.8,  # Flag if we approached timeout
                        "request_id": request_id
                    }
                )

                # Also log the query context
                if session_id:  # Only log context if session was created successfully
                    postgres_client.log_query_context(
                        original_question=request.question,
                        session_id=session_id,
                        highlight_override=request.highlight_override,
                        retrieved_chunks=retrieved_contexts,
                        processed_context=result.get("assembled_context", "")
                    )

                # Log how long logging took
                log_duration = time.time() - log_start
                if log_duration > 0.5:  # Log warning if logging takes more than 0.5s
                    print(f"WARNING: Session logging took {log_duration:.3f}s for request {request_id}")

            except Exception as e:
                print(f"ERROR: Error logging session in background for request {request_id}: {e}")

        background_tasks.add_task(log_session)

        # Performance monitoring
        total_time = time.time() - start_time
        if total_time > 2.0:
            logger.warning(f"Answer endpoint performance threshold exceeded", {
                "request_id": request_id,
                "response_time": round(total_time, 3),
                "threshold": 2.0
            })
        elif total_time > 1.5:
            logger.info(f"Answer endpoint approaching performance threshold", {
                "request_id": request_id,
                "response_time": round(total_time, 3),
                "threshold": 1.5
            })
        else:
            logger.info(f"Answer endpoint completed successfully", {
                "request_id": request_id,
                "response_time": round(total_time, 3)
            })

        return response

    except HTTPException:
        # Re-raise HTTP exceptions as they are already properly formatted
        raise
    except ValueError as e:
        # Handle validation errors
        logger.error(f"Validation error in answer endpoint", {
            "request_id": request_id,
            "error": str(e)
        })
        raise HTTPException(status_code=422, detail={
            "error": "Validation error",
            "details": str(e),
            "request_id": request_id
        })
    except Exception as e:
        # Log the error with timing information
        elapsed_time = time.time() - start_time
        logger.error(f"Unexpected error in answer endpoint", {
            "request_id": request_id,
            "elapsed_time": round(elapsed_time, 3),
            "error": str(e),
            "error_type": type(e).__name__
        })

        # Check if the error is related to Qdrant or other services being unavailable
        try:
            rag_agent = RAGAgent()
            if not rag_agent.retriever.qdrant_client.health_check():
                logger.warning(f"Qdrant service unavailable", {
                    "request_id": request_id
                })
                raise HTTPException(status_code=503, detail={
                    "error": "Service temporarily unavailable",
                    "details": "AI service is currently down, using limited functionality",
                    "request_id": request_id
                })
        except Exception as health_check_error:
            logger.warning(f"Could not check service health", {
                "request_id": request_id,
                "error": str(health_check_error)
            })

        # Return a user-friendly error message
        raise HTTPException(status_code=500, detail={
            "error": "Internal server error",
            "details": "An unexpected error occurred while processing your request",
            "request_id": request_id
        })


def _assemble_context_for_llm(question: str, retrieved_contexts: List[Dict]) -> str:
    """
    Assemble the context that would be sent to the LLM.

    Args:
        question: The original question
        retrieved_contexts: List of retrieved contexts

    Returns:
        Assembled context string
    """
    context_parts = ["Here is the relevant context from the textbook:"]

    for i, ctx in enumerate(retrieved_contexts, 1):
        source = ctx["metadata"].get("source_file", "unknown source")
        context_parts.append(f"{i}. From {source}: {ctx['content']}")

    context_parts.append(f"\nThe user's question is: {question}")
    context_parts.append("Please provide a comprehensive answer based on the above context.")

    return "\n\n".join(context_parts)


def _generate_answer_from_contexts(question: str, retrieved_contexts: List[Dict]) -> str:
    """
    Generate an answer based on the question and retrieved contexts.
    This is a placeholder implementation - in a real system, this would call an LLM.

    Args:
        question: The user's question
        retrieved_contexts: List of retrieved contexts

    Returns:
        Generated answer string
    """
    if not retrieved_contexts:
        return "I couldn't find any relevant information in the textbook to answer your question."

    # In a real implementation, this would send the question and contexts to an LLM
    # For now, we'll create a simulated response based on the contexts
    context_snippets = [ctx["content"][:200] + "..." for ctx in retrieved_contexts[:2]]  # Take first 2 contexts, first 200 chars
    return f"Based on the textbook content, here's an answer to your question '{question[:50]}...': " + \
           f"The text mentions: {'; '.join(context_snippets)}. " + \
           "This provides relevant information to address your query."


def _calculate_confidence_score(retrieved_contexts: List[Dict]) -> float:
    """
    Calculate a confidence score based on the retrieved contexts.

    Args:
        retrieved_contexts: List of retrieved contexts

    Returns:
        Confidence score between 0 and 1
    """
    if not retrieved_contexts:
        return 0.1  # Low confidence if no contexts found

    # Calculate average score of retrieved contexts
    avg_score = sum(ctx["score"] for ctx in retrieved_contexts) / len(retrieved_contexts)

    # Normalize the score to be between 0 and 1
    # Assuming similarity scores are between 0 and 1
    return min(1.0, max(0.0, avg_score))


@router.get("/answer/health",
            summary="Answer endpoint health check",
            description="Health check for the answer functionality.")
async def answer_health():
    """
    Health check for the answer functionality.

    Returns:
        dict: Health status of the answer service

    Raises:
        HTTPException: If the answer service is unavailable
    """
    try:
        rag_agent = RAGAgent()
        stats = rag_agent.get_retrieval_stats()

        return {
            "status": "healthy",
            "service": "answer-endpoint",
            "retrieval_stats": stats
        }
    except Exception as e:
        raise HTTPException(status_code=503, detail={
            "error": "Answer service unavailable",
            "details": str(e)
        })


def validate_answer_request(request: AnswerRequest) -> bool:
    """
    Validate the answer request parameters.

    Args:
        request: AnswerRequest object to validate

    Returns:
        True if valid, raises exception if invalid
    """
    import re

    if not request.question or len(request.question.strip()) == 0:
        raise HTTPException(status_code=422, detail={
            "error": "Validation failed",
            "details": {"question": "Question is required and cannot be empty"}
        })

    if len(request.question) > 10000:
        raise HTTPException(status_code=422, detail={
            "error": "Validation failed",
            "details": {"question": "Question exceeds maximum length of 10000 characters"}
        })

    # Additional validation for potentially malicious content
    if re.search(r'<script|javascript:|on\w+\s*=', request.question, re.IGNORECASE):
        raise HTTPException(status_code=422, detail={
            "error": "Validation failed",
            "details": {"question": "Question contains potentially malicious content"}
        })

    if request.k and (request.k < 1 or request.k > 10):
        raise HTTPException(status_code=422, detail={
            "error": "Validation failed",
            "details": {"k": "k must be between 1 and 10"}
        })

    if request.highlight_override and len(request.highlight_override) > 5000:
        raise HTTPException(status_code=422, detail={
            "error": "Validation failed",
            "details": {"highlight_override": "Highlight override exceeds maximum length of 5000 characters"}
        })

    # Additional validation for highlight override
    if request.highlight_override and re.search(r'<script|javascript:|on\w+\s*=', request.highlight_override, re.IGNORECASE):
        raise HTTPException(status_code=422, detail={
            "error": "Validation failed",
            "details": {"highlight_override": "Highlight override contains potentially malicious content"}
        })

    return True