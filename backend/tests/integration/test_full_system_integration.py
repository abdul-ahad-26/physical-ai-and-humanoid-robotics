"""
Full System Integration Tests for RAG + Agentic Backend for AI-Textbook Chatbot.

This module tests the complete integration between all system components:
- API endpoints
- Agent orchestration
- Database clients
- Qdrant vector database
- Security and rate limiting
- Logging and monitoring
"""

import pytest
import asyncio
import time
from typing import Dict, Any, List
import uuid
import json
from datetime import datetime, timedelta

# Import all the components to be tested
from src.api.main import app
from src.agent.orchestrator import MainOrchestratorAgent
from src.agent.rag_agent import RAGAgent
from src.agent.indexing_agent import IndexingAgent
from src.agent.logging_agent import LoggingAgent
from src.db.qdrant_client import QdrantClientWrapper
from src.db.postgres_client import PostgresClient
from src.rag.retriever import Retriever
from src.rag.embedder import Embedder
from src.api.metrics import metrics_collector, alert_manager
from src.api.security_audit import security_audit_logger
from src.api.feature_flags import feature_flags


class TestFullSystemIntegration:
    """Integration tests for the complete system"""

    def setup_class(self):
        """Setup for integration tests"""
        self.orchestrator = MainOrchestratorAgent()
        self.rag_agent = RAGAgent()
        self.indexing_agent = IndexingAgent()
        self.logging_agent = LoggingAgent()
        self.qdrant_client = QdrantClientWrapper()
        self.postgres_client = PostgresClient()
        self.retriever = Retriever()
        self.embedder = Embedder()

        # Sample content for testing
        self.sample_content = """
        # Introduction to Machine Learning

        Machine learning is a method of data analysis that automates analytical model building.
        It is a branch of artificial intelligence based on the idea that systems can learn
        from data, identify patterns and make decisions with minimal human intervention.

        ## Types of Machine Learning

        There are three main types of machine learning:

        1. Supervised Learning
        2. Unsupervised Learning
        3. Reinforcement Learning

        Supervised learning algorithms are trained using labeled examples, which means
        that the input data is paired with the correct output.
        """

        self.test_source_file = f"test_ml_intro_{uuid.uuid4().hex[:8]}.md"
        self.test_metadata = {
            "source_file": self.test_source_file,
            "section": "Introduction",
            "document_type": "markdown"
        }

    def teardown_class(self):
        """Cleanup after integration tests"""
        # Clean up test data from Qdrant
        try:
            self.qdrant_client.delete_by_payload("metadata.source_file", self.test_source_file)
        except Exception as e:
            print(f"Warning: Could not clean up Qdrant data: {e}")

        # Clean up test data from PostgreSQL
        try:
            # Remove any test sessions
            # In a real implementation, you would have methods to clean up test data
            pass
        except Exception as e:
            print(f"Warning: Could not clean up PostgreSQL data: {e}")

    def test_complete_indexing_workflow(self):
        """Test the complete indexing workflow from content upload to retrieval"""
        print("Testing complete indexing workflow...")

        # 1. Index content using the indexing agent
        result = self.indexing_agent.index_content(
            content=self.sample_content,
            source_file=self.test_source_file,
            document_type="markdown",
            section="Introduction"
        )

        assert result["status"] == "success", f"Indexing failed: {result}"
        assert result["indexed_chunks"] > 0, f"No chunks indexed: {result}"
        print(f"‚úì Successfully indexed {result['indexed_chunks']} chunks")

        # 2. Verify content is stored in Qdrant
        retrieved_chunks = self.rag_agent.retrieve_content("machine learning", k=5)
        assert len(retrieved_chunks) > 0, "No content retrieved from Qdrant"
        assert any("machine learning" in chunk["content"].lower() for chunk in retrieved_chunks), \
            "Indexed content not found in retrieval"
        print("‚úì Content successfully stored in Qdrant and retrievable")

        # 3. Verify content can be retrieved with semantic search
        query_result = self.rag_agent.process_query("What is machine learning?", k=3)
        assert len(query_result["retrieved_contexts"]) > 0, "Query did not return any contexts"
        assert "machine learning" in query_result["assembled_context"].lower(), \
            "Query context does not contain expected content"
        print("‚úì Semantic search working correctly")

    def test_complete_qa_workflow(self):
        """Test the complete Q&A workflow from query to answer generation"""
        print("Testing complete Q&A workflow...")

        # Ensure content is indexed first
        if not self._verify_content_exists():
            # Index the content if not already present
            result = self.indexing_agent.index_content(
                content=self.sample_content,
                source_file=self.test_source_file,
                document_type="markdown",
                section="Introduction"
            )
            assert result["status"] == "success"

        # 1. Process query through orchestrator
        start_time = time.time()
        result = self.orchestrator.process_query(
            query="What are the types of machine learning?",
            k=3,
            highlight_override=None
        )
        processing_time = time.time() - start_time

        # 2. Verify result structure
        assert "answer" in result, "Missing answer in result"
        assert "retrieved_contexts" in result, "Missing retrieved_contexts in result"
        assert "confidence_score" in result, "Missing confidence_score in result"
        assert "assembled_context" in result, "Missing assembled_context in result"
        print("‚úì Q&A workflow completed with proper result structure")

        # 3. Verify response quality
        assert result["answer"], "Answer is empty"
        assert result["confidence_score"] >= 0.0, "Invalid confidence score"
        assert len(result["retrieved_contexts"]) > 0, "No contexts retrieved"
        print("‚úì Answer generated with supporting contexts and confidence score")

        # 4. Verify performance requirements
        assert processing_time <= 2.0, f"Processing time {processing_time}s exceeded 2s limit"
        print(f"‚úì Processing completed within time limit: {processing_time:.3f}s")

    def test_agent_coordination(self):
        """Test coordination between different agents"""
        print("Testing agent coordination...")

        # 1. Test RAG agent can retrieve content
        rag_result = self.rag_agent.process_query("machine learning", k=2)
        assert len(rag_result["retrieved_contexts"]) > 0, "RAG agent failed to retrieve content"
        print("‚úì RAG agent working correctly")

        # 2. Test indexing agent can add content
        new_content = "## New Section\nThis is additional content for testing."
        index_result = self.indexing_agent.index_content(
            content=new_content,
            source_file=f"test_additional_{uuid.uuid4().hex[:8]}.md",
            document_type="markdown"
        )
        assert index_result["status"] == "success", "Indexing agent failed to add content"
        print("‚úì Indexing agent working correctly")

        # 3. Test orchestrator can coordinate both
        coord_result = self.orchestrator.process_query("additional content", k=2)
        # This might not find the new content immediately depending on indexing time
        # So we'll just verify the orchestrator can process the query
        assert "answer" in coord_result or "retrieved_contexts" in coord_result
        print("‚úì Orchestrator coordinating agents correctly")

    def test_database_integration(self):
        """Test integration with database components"""
        print("Testing database integration...")

        # 1. Test PostgreSQL client functionality
        test_session_id = f"test_session_{uuid.uuid4().hex[:8]}"
        test_query = "What is machine learning?"
        test_response = "Machine learning is..."

        # Log a user session
        session_id = self.postgres_client.log_user_session(
            query=test_query,
            response=test_response,
            retrieved_context=[{"content": "test", "metadata": {}, "score": 0.9}]
        )

        assert session_id, "Failed to log user session"
        print("‚úì PostgreSQL client can log user sessions")

        # 2. Test logging agent integration
        log_result = self.logging_agent.log_interaction(
            session_id=session_id,
            query=test_query,
            response=test_response,
            retrieved_context=[{"content": "test", "metadata": {}, "score": 0.9}]
        )

        assert log_result["status"] == "success", "Logging agent failed to log interaction"
        print("‚úì Logging agent integrates with database correctly")

        # 3. Test agent execution logging
        exec_log_result = self.postgres_client.log_agent_execution(
            agent_id="test_agent",
            session_id=session_id,
            tool_calls=["test_tool"],
            input_params={"param": "value"},
            output_result="test_result",
            execution_time=0.1
        )

        assert exec_log_result is not None, "Failed to log agent execution"
        print("‚úì Agent execution logging working correctly")

    def test_security_and_monitoring_integration(self):
        """Test integration of security and monitoring components"""
        print("Testing security and monitoring integration...")

        # 1. Test security audit logging
        security_audit_logger.log_data_access(
            user_id="test_user",
            ip_address="127.0.0.1",
            endpoint="/api/v1/query",
            method="POST",
            resource_id="test_resource"
        )
        print("‚úì Security audit logging integrated")

        # 2. Test metrics collection
        metrics_collector.record_request("POST", "/api/v1/query", 0.250, 200)
        metrics_collector.record_agent_execution("RAGAgent", "process_query", 0.180, True)
        metrics_collector.record_error("test_error", "/api/v1/query")

        summary = metrics_collector.get_metrics_summary()
        assert "request_rate_per_second" in summary, "Metrics collection not working"
        print("‚úì Metrics collection integrated")

        # 3. Test alerting system
        alerts = alert_manager.check_alerts()
        # May not trigger alerts with test values, but should not error
        assert isinstance(alerts, list), "Alert manager not working correctly"
        print("‚úì Alerting system integrated")

    def test_feature_flags_integration(self):
        """Test integration with feature flags"""
        print("Testing feature flags integration...")

        # 1. Create a test feature flag
        feature_name = f"integration_test_feature_{uuid.uuid4().hex[:8]}"
        feature_flags.create_flag(
            name=feature_name,
            enabled=True,
            rollout_percentage=100.0,
            description="Test feature for integration"
        )

        # 2. Test that feature is enabled
        is_enabled = feature_flags.is_enabled(feature_name, user_id="test_user")
        assert is_enabled, f"Feature {feature_name} should be enabled"
        print(f"‚úì Feature flag {feature_name} working correctly")

        # 3. Test gradual rollout
        feature_flags.set_rollout_percentage(feature_name, 50.0)
        # Note: With random hashing, this might not always be the same for the same user
        # but the system should handle percentage rollouts correctly
        print("‚úì Feature flag percentage rollout working")

    def test_error_handling_integration(self):
        """Test integration of error handling across components"""
        print("Testing error handling integration...")

        # 1. Test graceful degradation when Qdrant is potentially unavailable
        # (We won't actually shut down Qdrant, but test the fallback mechanisms)
        try:
            # This should work normally
            result = self.rag_agent.process_query("test query for error handling", k=1)
            assert "retrieved_contexts" in result
            print("‚úì Normal operation continues when services are available")
        except Exception as e:
            # If there's an error, it should be handled gracefully
            print(f"‚úì Error handled gracefully: {e}")

        # 2. Test database error handling
        try:
            # Attempt to log with invalid data (should be handled gracefully)
            invalid_result = self.postgres_client.log_user_session(
                query="",  # Empty query
                response="test",
                retrieved_context=[]
            )
            print("‚úì Database error handling working")
        except Exception:
            # Expected that invalid data might cause an error, but it should be handled
            print("‚úì Database error gracefully handled")

    def test_performance_under_load(self):
        """Test system performance under simulated load"""
        print("Testing performance under load...")

        # 1. Test concurrent requests
        async def simulate_concurrent_queries():
            tasks = []
            for i in range(5):  # Simulate 5 concurrent queries
                task = asyncio.create_task(
                    asyncio.to_thread(
                        self.orchestrator.process_query,
                        query=f"Test query {i} about machine learning",
                        k=2
                    )
                )
                tasks.append(task)

            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Verify all completed successfully
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"Query {i} failed: {result}")
                else:
                    assert "answer" in result or "retrieved_contexts" in result, \
                        f"Query {i} did not return expected result: {result}"

            return results

        # Run the concurrent test
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            results = loop.run_until_complete(simulate_concurrent_queries())
            successful_queries = sum(1 for r in results if not isinstance(r, Exception))
            print(f"‚úì Successfully processed {successful_queries}/5 concurrent queries")
        finally:
            loop.close()

    def test_complete_user_journey(self):
        """Test a complete user journey from content ingestion to Q&A"""
        print("Testing complete user journey...")

        # 1. Create new content
        textbook_content = """
        # Chapter 3: Deep Learning Fundamentals

        Deep learning is part of a broader family of machine learning methods
        based on artificial neural networks with representation learning.

        ## Neural Networks

        Artificial neural networks (ANNs) are computing systems vaguely inspired
        by the biological neural networks that constitute animal brains.

        ## Backpropagation

        Backpropagation is the key algorithm that makes deep learning possible.
        It calculates the gradient of the loss function with respect to the weights.
        """

        content_source = f"test_deep_learning_{uuid.uuid4().hex[:8]}.md"

        # 2. Index the content
        index_result = self.indexing_agent.index_content(
            content=textbook_content,
            source_file=content_source,
            document_type="markdown"
        )
        assert index_result["status"] == "success"
        print("‚úì Content indexed successfully")

        # 3. Query the content
        query_result = self.orchestrator.process_query(
            query="What is backpropagation?",
            k=3
        )

        assert query_result["answer"], "No answer generated for query"
        assert "backpropagation" in query_result["answer"].lower() or \
               "backpropagation" in query_result["assembled_context"].lower(), \
               "Answer does not contain expected content about backpropagation"
        print("‚úì Query answered with relevant content")

        # 4. Verify session was logged
        # Note: In a real test, we would verify that the session was properly logged
        # This is a simplified check
        assert "retrieved_contexts" in query_result
        print("‚úì User interaction properly processed through all components")

    def _verify_content_exists(self) -> bool:
        """Helper to verify if test content exists"""
        try:
            results = self.rag_agent.retrieve_content("machine learning", k=1)
            return len(results) > 0
        except:
            return False

    def test_cleanup_and_validation(self):
        """Final validation and cleanup"""
        print("Running final validation...")

        # Verify all systems are responsive
        assert self.qdrant_client.health_check(), "Qdrant health check failed"
        assert self.postgres_client.health_check(), "PostgreSQL health check failed"

        # Verify orchestrator health
        health = self.orchestrator.health_check()
        assert health["status"] in ["healthy", "degraded"], f"Orchestrator health check failed: {health}"

        print("‚úì All systems validated as healthy")


# Additional integration tests for API layer
class TestAPIIntegration:
    """Integration tests for the API layer with all components"""

    def test_api_endpoint_integration(self):
        """Test that API endpoints properly integrate with backend components"""
        print("Testing API endpoint integration...")

        # This would typically involve making actual HTTP requests to the FastAPI app
        # For now, we'll test the internal components that the API endpoints use

        orchestrator = MainOrchestratorAgent()

        # Test the same functionality that the API endpoints use
        result = orchestrator.process_query(
            query="Integration test query",
            k=2
        )

        assert "answer" in result or "retrieved_contexts" in result
        print("‚úì API backend components working correctly")


# Run all integration tests
def run_integration_tests():
    """Run all integration tests"""
    test_instance = TestFullSystemIntegration()

    try:
        test_instance.setup_class()

        # Run all tests
        test_instance.test_complete_indexing_workflow()
        test_instance.test_complete_qa_workflow()
        test_instance.test_agent_coordination()
        test_instance.test_database_integration()
        test_instance.test_security_and_monitoring_integration()
        test_instance.test_feature_flags_integration()
        test_instance.test_error_handling_integration()
        test_instance.test_performance_under_load()
        test_instance.test_complete_user_journey()
        test_instance.test_cleanup_and_validation()

        # API integration tests
        api_test = TestAPIIntegration()
        api_test.test_api_endpoint_integration()

        print("\nüéâ All integration tests passed!")
        return True

    except Exception as e:
        print(f"\n‚ùå Integration test failed: {e}")
        raise
    finally:
        try:
            test_instance.teardown_class()
            print("‚úÖ Cleanup completed")
        except Exception as e:
            print(f"‚ö†Ô∏è  Cleanup error: {e}")


if __name__ == "__main__":
    run_integration_tests()